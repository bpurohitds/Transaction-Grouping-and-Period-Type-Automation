{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "019328e7-9bfa-4a3e-b7d0-fb77b0fddd61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All available files were loaded and concatenated successfully.\n",
            "Final DataFrame shape: (659138, 2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# File paths for the Excel files\n",
        "file_DLL = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Deferred Lines List INDIAMART_SMALIK.xlsx\"\n",
        "file_DR = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Extracted Jan-25 DR.xlsb\"\n",
        "file_RR = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Revenue Reversal INDIAMART_SMALIK 2025-02-03T09_17_49.xlsx\"\n",
        "file_DRP = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Direct revenue Posting OT INDIAMART_SMALIK 2024-11-04T03_39_58.xlsx\"\n",
        "\n",
        "# List of the columns to read\n",
        "columns_to_read = ['Renewal Entry No.', 'Approved Performa']\n",
        "\n",
        "# Function to read Excel files safely\n",
        "def read_excel_file(file_path, columns):\n",
        "    try:\n",
        "        if os.path.exists(file_path):  # Check if file exists\n",
        "            if file_path.endswith('.xlsb'):\n",
        "                return pd.read_excel(file_path, usecols=columns, engine='pyxlsb')\n",
        "            else:\n",
        "                return pd.read_excel(file_path, usecols=columns)\n",
        "        else:\n",
        "            print(f\"⚠️ Warning: File not found - {file_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error reading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Read each file into a dataframe if it exists\n",
        "dataframes = []\n",
        "\n",
        "for file in [file_DLL, file_DR, file_RR, file_DRP]:\n",
        "    df = read_excel_file(file, columns_to_read)\n",
        "    if df is not None:\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Check if we have data to concatenate\n",
        "if dataframes:\n",
        "    consol_df = pd.concat(dataframes, ignore_index=True)\n",
        "    print('✅ All available files were loaded and concatenated successfully.')\n",
        "else:\n",
        "    consol_df = pd.DataFrame()  # Empty DataFrame if no files were loaded\n",
        "    print('⚠️ No valid files found. Empty DataFrame created.')\n",
        "\n",
        "# Print dataframe shape to verify\n",
        "print(f\"Final DataFrame shape: {consol_df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "00a19dc2-1f5c-4748-97b9-9cbc548610e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicates before removing duplicates: 286757\n",
            "Number of duplicates after removing duplicates: 0\n",
            "   Renewal Entry No.  Approved Performa\n",
            "0          2299943.0          1881940.0\n",
            "1          3486879.0          2722964.0\n",
            "2          3598174.0          2838382.0\n",
            "3          3807460.0          3056590.0\n",
            "4          4092785.0          3354887.0\n",
            "✅Done! File saved at D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Unique_after_Stacked.csv\n"
          ]
        }
      ],
      "source": [
        "# Print the number of duplicates before removing duplicates\n",
        "duplicates_before = consol_df.duplicated().sum()\n",
        "print(f\"Number of duplicates before removing duplicates: {duplicates_before}\")\n",
        "\n",
        "# Remove duplicates from the consolidated dataframe\n",
        "consol_df = consol_df.drop_duplicates()\n",
        "\n",
        "# Remove rows with any NaN or blank values in any column\n",
        "consol_df = consol_df.dropna()  # Removes rows with NaN values\n",
        "consol_df = consol_df[~(consol_df.astype(str).apply(lambda x: x.str.strip()) == '').any(axis=1)]  # Removes blank values\n",
        "\n",
        "# Print the number of duplicates after removing duplicates\n",
        "duplicates_after = consol_df.duplicated().sum()\n",
        "print(f\"Number of duplicates after removing duplicates: {duplicates_after}\")\n",
        "\n",
        "# Display the consolidated dataframe\n",
        "print(consol_df.head())\n",
        "\n",
        "# Save unique cases after merging all DFs\n",
        "output_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Unique_after_Stacked.csv\"\n",
        "consol_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f'✅Done! File saved at {output_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a13b4d4-2839-42e8-a109-5ef7809caf74",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows after removing duplicates: 372380\n"
          ]
        }
      ],
      "source": [
        "# Count of rows after duplicates are removed\n",
        "rows_after_deduplication = len(consol_df)\n",
        "print(f\"Number of rows after removing duplicates: {rows_after_deduplication}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c227542f-d82c-426a-878c-85a6eb2a00c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Renewal Entry No. Approved Performa          Already Grouped?\n",
            "0           2299943           1881940  SUBSCRIPTION-CATALOG-MYR\n",
            "1           3486879           2722964                    OTHERS\n",
            "2           3598174           2838382                    OTHERS\n",
            "3           3807460           3056590                    OTHERS\n",
            "4           4092785           3354887  SUBSCRIPTION-LISTING-MYR\n",
            "✅Consol DF saved\n"
          ]
        }
      ],
      "source": [
        "# Define the file path\n",
        "file_DL = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Deferred List INDIAMART_ANUJ 2025-02-03.xlsx\"\n",
        "\n",
        "# Read all sheets into a dictionary of DataFrames\n",
        "DL_sheets = pd.read_excel(file_DL, sheet_name=None)  # Load all sheets\n",
        "\n",
        "# Columns needed for lookup\n",
        "columns_to_read = ['Renewal Entry No.', 'Service Group']\n",
        "\n",
        "# Combine only the required columns from all sheets\n",
        "DL_combined = pd.concat(\n",
        "    [df[columns_to_read] for df in DL_sheets.values()], \n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Fill NaN values before converting to int and string\n",
        "consol_df['Renewal Entry No.'] = consol_df['Renewal Entry No.'].fillna(0).astype(int).astype(str)\n",
        "consol_df['Approved Performa'] = consol_df['Approved Performa'].fillna(0).astype(int).astype(str)\n",
        "DL_combined['Renewal Entry No.'] = DL_combined['Renewal Entry No.'].fillna(0).astype(int).astype(str)\n",
        "\n",
        "# Perform the lookup and update 'Already Grouped?' with 'Service Group' values\n",
        "consol_df = consol_df.merge(DL_combined, on='Renewal Entry No.', how='left')\n",
        "\n",
        "# Replace missing values in 'Service Group' with '#NA'\n",
        "consol_df['Service Group'] = consol_df['Service Group'].fillna('#NA')\n",
        "\n",
        "# Rename column to 'Already Grouped?'\n",
        "consol_df.rename(columns={'Service Group': 'Already Grouped?'}, inplace=True)\n",
        "\n",
        "#Remove Duplicates\n",
        "consol_df = consol_df.drop_duplicates()\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(consol_df.head())\n",
        "\n",
        "#Save consol_df\n",
        "consol_df.to_csv(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Consol_df.csv\", index=False)\n",
        "print('✅Consol DF saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ba617fbf-f70f-42fc-a0db-08e5fc2a9c40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅Consol_df_NA_Cases saved at D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Consol_df_NA_Cases.csv\n"
          ]
        }
      ],
      "source": [
        "# Filter only rows where 'Already Grouped?' is '#NA'\n",
        "consol_df_NA = consol_df[consol_df['Already Grouped?'] == '#NA'][['Approved Performa', 'Already Grouped?']]\n",
        "\n",
        "# Remove duplicates\n",
        "consol_df_NA = consol_df_NA.drop_duplicates()\n",
        "\n",
        "#Count of #NA Cases\n",
        "consol_df_NA.count()\n",
        "\n",
        "# Save the filtered dataframe\n",
        "na_output_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Consol_df_NA_Cases.csv\"\n",
        "consol_df_NA.to_csv(na_output_path, index=False)\n",
        "\n",
        "print(f'✅Consol_df_NA_Cases saved at {na_output_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "273ead80-1820-4c12-9153-994c99e2f2ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Approved Performa           Remark\n",
            "405303           8817820  Receipt Tagging\n",
            "405343           8695698  Receipt Tagging\n",
            "422796           8829963  Receipt Tagging\n",
            "434600           8843269  Receipt Tagging\n",
            "451279           8853654  Receipt Tagging\n",
            "✅Done! File saved at D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Receipt tagging extract.csv\n"
          ]
        }
      ],
      "source": [
        "# Rename column 'Already Grouped?' to 'Remark'\n",
        "consol_df_NA.rename(columns={'Already Grouped?': 'Remark'}, inplace=True)\n",
        "\n",
        "# Fill 'Remark' column with 'Receipt Tagging'\n",
        "consol_df_NA['Remark'] = 'Receipt Tagging'\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(consol_df_NA.head())\n",
        "\n",
        "# Save the updated dataframe\n",
        "output_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Receipt tagging extract.csv\"\n",
        "consol_df_NA.to_csv(output_path, index=False)\n",
        "\n",
        "print(f'✅Done! File saved at {output_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6141923e-7ec2-44ee-841b-61f9bbc8126d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found sheet: TaggingData_04.02.25 04.08 PM\n",
            "   Document No. Tagging Type  Approved Performa  Receipt Type        Remarks\n",
            "0       3493657      Receipt            2413186           3.0  Other tagging\n",
            "1       3522759      Receipt            2413186           6.0             CN\n",
            "2       3489899      Receipt            2413186           5.0  Other tagging\n",
            "3       3266970      Receipt            2415514           2.0  Other tagging\n",
            "4       6024590      Receipt            4605836           3.0  Other tagging\n",
            "✅ Tagging data updated successfully and saved at D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\TaggingData_Updated.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\2. TaggingData_04.02.25 04.08 PM Jan.xlsb\"\n",
        "\n",
        "# Get all sheet names\n",
        "if file_path.endswith('.xlsb'):\n",
        "    sheet_names = pd.ExcelFile(file_path, engine='pyxlsb').sheet_names\n",
        "else:\n",
        "    sheet_names = pd.ExcelFile(file_path, engine='openpyxl').sheet_names\n",
        "\n",
        "# Find the sheet containing \"TaggingData\"\n",
        "tagging_sheet = next((sheet for sheet in sheet_names if \"TaggingData\" in sheet), None)\n",
        "\n",
        "# Ensure the sheet exists\n",
        "if tagging_sheet:\n",
        "    print(f\"✅ Found sheet: {tagging_sheet}\")\n",
        "\n",
        "    # Read the identified sheet\n",
        "    if file_path.endswith('.xlsb'):\n",
        "        df_tagging = pd.read_excel(file_path, sheet_name=tagging_sheet, engine='pyxlsb')\n",
        "    else:\n",
        "        df_tagging = pd.read_excel(file_path, sheet_name=tagging_sheet, engine='openpyxl')\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    if 'Receipt Type' in df_tagging.columns and 'Tagging Type' in df_tagging.columns:\n",
        "        \n",
        "        # Initialize 'Remarks' column\n",
        "        df_tagging['Remarks'] = 'Other tagging'  # Default value\n",
        "\n",
        "        # Condition 1: If 'Receipt Type' is in (6, 25, 27, 28) → Update to 'CN'\n",
        "        df_tagging.loc[df_tagging['Receipt Type'].isin([6, 25, 27, 28]), 'Remarks'] = 'CN'\n",
        "\n",
        "        # Condition 2: If 'Tagging Type' is 'Adjustment Note' → Update to 'ADJN'\n",
        "        df_tagging.loc[df_tagging['Tagging Type'] == 'Adjustment Note', 'Remarks'] = 'ADJN'\n",
        "\n",
        "        # Display updated dataframe\n",
        "        print(df_tagging.head())\n",
        "\n",
        "        # Save the updated file\n",
        "        output_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\TaggingData_Updated.csv\"\n",
        "        df_tagging.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"✅ Tagging data updated successfully and saved at {output_path}\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ 'Receipt Type' or 'Tagging Type' column not found in the dataset!\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No sheet containing 'TaggingData' found in the file!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ed9afa81-c182-4fa4-a7fd-8724ece6700c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅pivot_df_tagging downloaded\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert \"Approved Performa\" to numeric\n",
        "df_tagging[\"Approved Performa\"] = pd.to_numeric(df_tagging[\"Approved Performa\"], errors=\"coerce\")\n",
        "\n",
        "# Create the pivot table\n",
        "pivot_df_tagging = df_tagging.pivot_table(\n",
        "    index=\"Approved Performa\",\n",
        "    columns=\"Remarks\",\n",
        "    values=\"Document No.\",  # Changed to Document No. to avoid issues\n",
        "    aggfunc=\"count\",\n",
        "    fill_value=0,\n",
        ")\n",
        "\n",
        "    # Ensure all expected columns exist\n",
        "expected_columns = [\"ADJN\", \"CN\", \"Other tagging\"]  # Only required columns\n",
        "for col in expected_columns:\n",
        "    if col not in pivot_df_tagging:\n",
        "        pivot_df_tagging[col] = 0  # Add missing columns\n",
        "\n",
        "# Add 'Grand Total' column (row-wise sum)\n",
        "pivot_df_tagging[\"Grand Total\"] = pivot_df_tagging[expected_columns].sum(axis=1)\n",
        "\n",
        "# Rename columns for clarity\n",
        "pivot_df_tagging.columns.name = \"Count of Remarks\"\n",
        "\n",
        "# Ensure the index is properly reset to avoid extra index column\n",
        "pivot_df_tagging.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Save the final pivot table to an Excel file\n",
        "pivot_df_tagging.to_excel(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\pivot_df_tagging.xlsx\", index=False)\n",
        "\n",
        "print(\"✅pivot_df_tagging downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c474a96c-73e6-4771-a8e4-69bb5778b54e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅pivot_df_tagging downloaded\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert \"Approved Performa\" to numeric\n",
        "df_tagging[\"Approved Performa\"] = pd.to_numeric(df_tagging[\"Approved Performa\"], errors=\"coerce\")\n",
        "\n",
        "# Create the pivot table\n",
        "pivot_df_tagging = df_tagging.pivot_table(\n",
        "    index=\"Approved Performa\",\n",
        "    columns=\"Remarks\",\n",
        "    values=\"Document No.\",  # Changed to Document No. to avoid issues\n",
        "    aggfunc=\"count\",\n",
        "    fill_value=0,\n",
        ")\n",
        "\n",
        "# Ensure all expected columns exist\n",
        "expected_columns = [\"ADJN\", \"CN\", \"Other tagging\"]  # Only required columns\n",
        "for col in expected_columns:\n",
        "    if col not in pivot_df_tagging:\n",
        "        pivot_df_tagging[col] = 0  # Add missing columns\n",
        "\n",
        "# Add 'Grand Total' column (row-wise sum)\n",
        "pivot_df_tagging[\"Grand Total\"] = pivot_df_tagging[expected_columns].sum(axis=1)\n",
        "\n",
        "# Corrected 'Relv/Not Relv' column condition\n",
        "pivot_df_tagging[\"Relv/Not Relv\"] = pivot_df_tagging.apply(\n",
        "    lambda row: \"R\" if (row[\"ADJN\"] != 0 or row[\"CN\"] != 0) and row[\"Other tagging\"] == 0 else \"NR\", axis=1\n",
        ")\n",
        "\n",
        "# Rename columns for clarity\n",
        "pivot_df_tagging.columns.name = \"Count of Remarks\"\n",
        "\n",
        "# Reset index if needed\n",
        "pivot_df_tagging.reset_index(inplace=True)\n",
        "\n",
        "# Save the final pivot table to an Excel file\n",
        "pivot_df_tagging.to_excel(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\pivot_df_tagging_relv_nonrelv.xlsx\", index=False)\n",
        "\n",
        "print(\"✅pivot_df_tagging downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f4cd77b8-b88c-4bbf-a79a-35c08e7b825d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅pivot_df_tagging downloaded\n"
          ]
        }
      ],
      "source": [
        "# Add 'Source for R/NR' column with the defined conditions\n",
        "def get_source_for_rnr(row):\n",
        "    if row[\"Relv/Not Relv\"] == \"NR\":\n",
        "        return \"Other Tagging\"\n",
        "    elif row[\"CN\"] != 0 and row[\"ADJN\"] == 0:\n",
        "        return \"Only CDN\"\n",
        "    elif row[\"ADJN\"] != 0 and row[\"CN\"] == 0:\n",
        "        return \"Only ADJN\"\n",
        "    elif row[\"CN\"] != 0 and row[\"ADJN\"] != 0 and row[\"Other tagging\"] == 0:\n",
        "        return \"1. Both CN/ADJN, No Other tagging\"\n",
        "    return \"\"\n",
        "\n",
        "pivot_df_tagging[\"Source for R/NR\"] = pivot_df_tagging.apply(get_source_for_rnr, axis=1)\n",
        "\n",
        "# Rename columns for clarity\n",
        "pivot_df_tagging.columns.name = \"Count of Remarks\"\n",
        "\n",
        "# Reset index if needed\n",
        "pivot_df_tagging.reset_index(inplace=True)\n",
        "\n",
        "# Save the final pivot table to an Excel file\n",
        "pivot_df_tagging.to_excel(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\pivot_df_tagging_source_added.xlsx\", index=False)\n",
        "\n",
        "print(\"✅pivot_df_tagging downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ea6091c5-aa4f-47b6-9bc3-3172d1aed65a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ CN_Amt_df created successfully!\n",
            "CN_Amt_df Saved\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Inbound Work Order Receipts INDIAMART_ANUJ 2025-02-04T11_10_39.xlsx\"\n",
        "\n",
        "# Define the sheet name\n",
        "sheet_name = \"Inbound Work Order Receipts\"\n",
        "\n",
        "# Load the Excel file (supports both .xlsx and .xlsb)\n",
        "try:\n",
        "    if file_path.endswith(\".xlsb\"):\n",
        "        # Use pyxlsb for .xlsb files\n",
        "        import pyxlsb\n",
        "        df_cn_amt = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"pyxlsb\")\n",
        "    else:\n",
        "        # Standard pandas read for .xlsx\n",
        "        df_cn_amt = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required_columns = [\"Approved Performa No\", \"Receipt Amount Used\"]\n",
        "    missing_columns = [col for col in required_columns if col not in df_cn_amt.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing columns in the file: {missing_columns}\")\n",
        "\n",
        "    # Select only the required columns\n",
        "    df_cn_amt = df_cn_amt[required_columns]\n",
        "\n",
        "    # Group by 'Approved Performa No' and sum 'Receipt Amount Used'\n",
        "    CN_Amt_df = df_cn_amt.groupby(\"Approved Performa No\", as_index=False).agg({\"Receipt Amount Used\": \"sum\"})\n",
        "\n",
        "    print(\"✅ CN_Amt_df created successfully!\")\n",
        "    #Save CN_AMT_Df\n",
        "    CN_Amt_df.to_csv(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\CN_Amt_df.csv\", index=False)\n",
        "    print('CN_Amt_df Saved')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading file: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "92abd8bb-27ec-4427-ae5e-7847d4247ab5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Lookup for 'Receipt Amount Used' completed successfully!\n",
            "CN_with_R Saved\n"
          ]
        }
      ],
      "source": [
        "# Filter pivot_df_tagging where 'Relv/Not Relv' == 'R' and 'CN' != 0\n",
        "filtered_df = pivot_df_tagging[(pivot_df_tagging[\"Relv/Not Relv\"] == \"R\") & (pivot_df_tagging[\"CN\"] != 0)]\n",
        "\n",
        "# Perform lookup using 'Approved Performa' as the key\n",
        "pivot_df_tagging[\"Receipt Amount Used\"] = pivot_df_tagging[\"Approved Performa\"].map(\n",
        "    CN_Amt_df.set_index(\"Approved Performa No\")[\"Receipt Amount Used\"]\n",
        ")\n",
        "\n",
        "# Keep 'Receipt Amount Used' only for filtered cases, set others to NaN\n",
        "pivot_df_tagging.loc[~pivot_df_tagging.index.isin(filtered_df.index), \"Receipt Amount Used\"] = None\n",
        "\n",
        "print(\"✅ Lookup for 'Receipt Amount Used' completed successfully!\")\n",
        "\n",
        "#Save pivot_df_tagging\n",
        "\n",
        "pivot_df_tagging.to_csv(r'D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\CN_with_R.csv', index=False)\n",
        "print('CN_with_R Saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "517f97ae-b406-4145-b82d-44a14c01c4c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Columns renamed and 'CN Tagged Amt Ex GST' calculated successfully!\n",
            "✅ CN Tagged Amt Ex GST Saved\n"
          ]
        }
      ],
      "source": [
        "# Rename 'Receipt Amount Used' to 'CN Amt For R Cases'\n",
        "pivot_df_tagging.rename(columns={\"Receipt Amount Used\": \"CN Amt For R Cases\"}, inplace=True)\n",
        "\n",
        "# Calculate 'CN Tagged Amt Ex GST' as 'CN Amt For R Cases' / 1.18\n",
        "pivot_df_tagging[\"CN Tagged Amt Ex GST\"] = pivot_df_tagging[\"CN Amt For R Cases\"] / 1.18\n",
        "\n",
        "print(\"✅ Columns renamed and 'CN Tagged Amt Ex GST' calculated successfully!\")\n",
        "\n",
        "#Save file\n",
        "pivot_df_tagging.to_csv(r'D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\CN Tagged Amt Ex GST.csv', index=False)\n",
        "print('✅ CN Tagged Amt Ex GST Saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "31b4ce1d-7491-4c87-a368-3cbe16e9156d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ADJN_Amt_df created successfully!\n",
            "✅ ADJN_Amt_df Saved!\n",
            "✅ Lookup for 'Base Amount' completed successfully!\n",
            "✅ Columns renamed and 'Total Amount' calculated successfully!\n",
            "✅ ADJN_Only_Cases_Amt_df Saved!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\Inbnd Adjustment Note Appl. INDIAMART_ANUJ 2025-02-04T10_42_43.xlsx\"\n",
        "\n",
        "# Define the sheet name\n",
        "sheet_name = \"Inbnd Adjustment Note Appl.\"\n",
        "\n",
        "# Load the Excel file (supports both .xlsx and .xlsb)\n",
        "try:\n",
        "    if file_path.endswith(\".xlsb\"):\n",
        "        # Use pyxlsb for .xlsb files\n",
        "        import pyxlsb\n",
        "        ADJN_Amt_df = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"pyxlsb\")\n",
        "    else:\n",
        "        # Standard pandas read for .xlsx\n",
        "        ADJN_Amt_df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required_columns = [\"Approved Performa\", \"Base Amount\"]\n",
        "    missing_columns = [col for col in required_columns if col not in ADJN_Amt_df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"❌ Missing columns in the file: {missing_columns}\")\n",
        "\n",
        "    # Select only the required columns\n",
        "    ADJN_Amt_df = ADJN_Amt_df[required_columns]\n",
        "\n",
        "    # Group by 'Approved Performa' and sum 'Base Amount'\n",
        "    ADJN_Amt_df = ADJN_Amt_df.groupby(\"Approved Performa\", as_index=False).agg({\"Base Amount\": \"sum\"})\n",
        "\n",
        "    print(\"✅ ADJN_Amt_df created successfully!\")\n",
        "\n",
        "    # Save ADJN_Amt_df\n",
        "    ADJN_Amt_df.to_csv(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\ADJN_Amt_df.csv\", index=False)\n",
        "    print(\"✅ ADJN_Amt_df Saved!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading file: {e}\")\n",
        "\n",
        "# ------------------------------------\n",
        "# 🔹 Lookup 'Base Amount' for 'ADJN' cases\n",
        "# ------------------------------------\n",
        "\n",
        "# Filter pivot_df_tagging where 'Source for R/NR' is in the given list\n",
        "filtered_df = pivot_df_tagging[pivot_df_tagging[\"Source for R/NR\"].isin([\"Only ADJN\", \"1. Both CN/ADJN, No Other tagging\"])]\n",
        "\n",
        "# Perform lookup using 'Approved Performa' as the key\n",
        "pivot_df_tagging[\"Base Amount\"] = pivot_df_tagging[\"Approved Performa\"].map(\n",
        "    ADJN_Amt_df.set_index(\"Approved Performa\")[\"Base Amount\"]\n",
        ")\n",
        "# Set 'Base Amount' to NaN for non-matching cases\n",
        "pivot_df_tagging.loc[~pivot_df_tagging.index.isin(filtered_df.index), \"Base Amount\"] = None\n",
        "\n",
        "print(\"✅ Lookup for 'Base Amount' completed successfully!\")\n",
        "\n",
        "# Rename 'Base Amount' to 'ADJN Appl Amt'\n",
        "pivot_df_tagging.rename(columns={\"Base Amount\": \"ADJN Appl Amt\"}, inplace=True)\n",
        "\n",
        "print(\"✅ Columns renamed and 'Total Amount' calculated successfully!\")\n",
        "\n",
        "# Add 'Total Amount' column (sum of 'CN Tagged Amt Ex GST' + 'ADJN Appl Amt' and fill 0 where value is null)\n",
        "pivot_df_tagging[\"Total Amount\"] = pivot_df_tagging[\"CN Tagged Amt Ex GST\"].fillna(0) + pivot_df_tagging[\"ADJN Appl Amt\"].fillna(0)\n",
        "\n",
        "# Save the updated pivot_df_tagging\n",
        "pivot_df_tagging.to_csv(r\"D:\\Bhuwan Data\\Bhuwan\\Desktop\\Adhoc data of BPnA team\\Sonali\\Grouping\\ADJN_Only_Cases_Amt_df.csv\", index=False)\n",
        "print(\"✅ ADJN_Only_Cases_Amt_df Saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "dd38a115-af06-477a-a3cc-d865ce87164d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅Automation Done till here😄😄🎉\n"
          ]
        }
      ],
      "source": [
        "print(\"✅Automation Done till here😄😄🎉\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "528577e3-7aa2-463f-aae1-2307f8322aa7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}